{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras\n",
      "  Downloading Keras-2.4.3-py2.py3-none-any.whl (36 kB)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.3)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.10.0)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.4.1)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (5.3.1)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->keras) (1.15.0)\n",
      "Installing collected packages: keras\n",
      "Successfully installed keras-2.4.3\n",
      "\u001b[33mWARNING: You are using pip version 20.2.3; however, version 21.1.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy as sp\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import heapq\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_rating_file_as_list(filename):\n",
    "    rating_list = []\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        \n",
    "        while line and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item = int(arr[0]), int(arr[1])\n",
    "            rating_list.append([user, item])\n",
    "            line = f.readline()\n",
    "    \n",
    "    return rating_list\n",
    "\n",
    "def load_negative_file(filename):\n",
    "    negative_list = []\n",
    "    \n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        \n",
    "        while line and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            negatives = []\n",
    "            \n",
    "            for x in arr[1:]:\n",
    "                negatives.append(int(x))\n",
    "            \n",
    "            negative_list.append(negatives)\n",
    "            \n",
    "            line = f.readline()\n",
    "    \n",
    "    return negative_list\n",
    "\n",
    "def load_rating_file_as_matrix(filename):\n",
    "    num_users, num_items = 0, 0\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            u, i = int(arr[0]), int(arr[1])\n",
    "            num_users = max(num_users, u)\n",
    "            num_items = max(num_items, i)\n",
    "            line = f.readline()\n",
    "    \n",
    "    mat = sp.sparse.dok_matrix((num_users+1, num_items+1), dtype=np.float32)\n",
    "    with open(filename, \"r\") as f:\n",
    "        line = f.readline()\n",
    "        while line != None and line != \"\":\n",
    "            arr = line.split(\"\\t\")\n",
    "            user, item, rating = int(arr[0]), int(arr[1]), float(arr[2])\n",
    "            if (rating > 0):\n",
    "                mat[user, item] = 1.0\n",
    "            line = f.readline()    \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = load_rating_file_as_matrix('./Data/ml-1m/ml-1m.train.rating')\n",
    "test_ratings = load_rating_file_as_list('./Data/ml-1m/ml-1m.test.rating')\n",
    "test_negatives = load_negative_file('./Data/ml-1m/ml-1m.test.negative')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded Data. # Users: 6040 # Items: 3706 # Train: 994169 # Test: 6040\n"
     ]
    }
   ],
   "source": [
    "num_users, num_items = train.shape\n",
    "print('Loaded Data. # Users:', num_users, '# Items:', num_items, '# Train:', train.nnz, '# Test:', len(test_ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper to Generate Negative Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_instances(train, num_negatives):\n",
    "    user_input, item_input, labels = [],[],[]\n",
    "    num_users = train.shape[0]\n",
    "    for (u, i) in train.keys():\n",
    "        # positive instance\n",
    "        user_input.append(u)\n",
    "        item_input.append(i)\n",
    "        labels.append(1)\n",
    "        # negative instances\n",
    "        for t in range(num_negatives):\n",
    "            j = np.random.randint(num_items)\n",
    "            while train.get((u, j)):\n",
    "                j = np.random.randint(num_items)\n",
    "            user_input.append(u)\n",
    "            item_input.append(j)\n",
    "            labels.append(0)\n",
    "    return user_input, item_input, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Embedding, Input, Dense, Reshape, Multiply, Flatten, Lambda, Concatenate, Conv2D, MaxPool2D\n",
    "from keras import initializers, regularizers\n",
    "import sys\n",
    "\n",
    "def get_OuterProductmodel(num_users, num_items, latent_dim):\n",
    "    user_input = Input(shape = (1,), dtype = 'int32', name = 'user')\n",
    "    item_input = Input(shape = (1,), dtype = 'int32', name = 'item')\n",
    "\n",
    "    user_embedding = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embed',\n",
    "                             embeddings_initializer = initializers.RandomNormal(stddev = 0.01), \n",
    "                             embeddings_regularizer = regularizers.l2(0), input_length = 1)\n",
    "    item_embedding = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embed',\n",
    "                             embeddings_initializer = initializers.RandomNormal(stddev = 0.01), \n",
    "                             embeddings_regularizer = regularizers.l2(0), input_length = 1)\n",
    "\n",
    "    user_latent = Flatten()(user_embedding(user_input))\n",
    "    item_latent = Flatten()(item_embedding(item_input))\n",
    "\n",
    "    latent_map = tf.linalg.matmul(tf.expand_dims(user_latent, -1), tf.expand_dims(item_latent, 1))\n",
    "\n",
    "    x = tf.expand_dims(latent_map, -1)\n",
    "\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = MaxPool2D(pool_size=2)(x)\n",
    "    x = Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = Flatten()(x)\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(x)\n",
    "\n",
    "    return Model(inputs=[user_input, item_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/building-a-resnet-in-keras-e8f1322a49ba\n",
    "def relu_bn(inputs):\n",
    "    relu = tf.keras.layers.ReLU()(inputs)\n",
    "    bn = tf.keras.layers.BatchNormalization()(relu)\n",
    "    return bn\n",
    "\n",
    "def residual_block(x, downsample = False, filters = 16, kernel_size = 3):\n",
    "    y = tf.keras.layers.Conv2D(kernel_size=kernel_size,\n",
    "               strides= (1 if not downsample else 2),\n",
    "               filters=filters,\n",
    "               padding=\"same\")(x)\n",
    "    y = relu_bn(y)\n",
    "    y = tf.keras.layers.Conv2D(kernel_size=kernel_size,\n",
    "               strides=1,\n",
    "               filters=filters,\n",
    "               padding=\"same\")(y)\n",
    "\n",
    "    if downsample:\n",
    "        x = tf.keras.layers.Conv2D(kernel_size=1,\n",
    "                   strides=2,\n",
    "                   filters=filters,\n",
    "                   padding=\"same\")(x)\n",
    "    out = tf.keras.layers.Add()([x, y])\n",
    "    out = relu_bn(out)\n",
    "    return out\n",
    "\n",
    "def get_ResidualModel(num_users, num_items, latent_dim):\n",
    "    user_input = Input(shape = (1,), dtype = 'int32', name = 'user')\n",
    "    item_input = Input(shape = (1,), dtype = 'int32', name = 'item')\n",
    "\n",
    "    user_embedding = Embedding(input_dim = num_users, output_dim = latent_dim, name = 'user_embed',\n",
    "                             embeddings_initializer = initializers.RandomNormal(stddev = 0.01), \n",
    "                             embeddings_regularizer = regularizers.l2(0), input_length = 1)\n",
    "    item_embedding = Embedding(input_dim = num_items, output_dim = latent_dim, name = 'item_embed',\n",
    "                             embeddings_initializer = initializers.RandomNormal(stddev = 0.01), \n",
    "                             embeddings_regularizer = regularizers.l2(0), input_length = 1)\n",
    "\n",
    "    user_latent = Flatten()(user_embedding(user_input))\n",
    "    item_latent = Flatten()(item_embedding(item_input))\n",
    "\n",
    "    latent_map = tf.linalg.matmul(tf.expand_dims(user_latent, -1), tf.expand_dims(item_latent, 1))\n",
    "\n",
    "    x = tf.expand_dims(latent_map, -1)\n",
    "    \n",
    "    layers = [2, 2, 2]\n",
    "    \n",
    "    for num_layers in layers:\n",
    "        for _ in range(num_layers):\n",
    "            x = residual_block(x, downsample = False, filters = 8, kernel_size = 3)\n",
    "        x = residual_block(x, downsample = True, filters = 8, kernel_size = 3)\n",
    "    \n",
    "    x = Flatten()(x)\n",
    "    prediction = Dense(1, activation='sigmoid', kernel_initializer='lecun_uniform', name = 'prediction')(x)\n",
    "    \n",
    "    return Model(inputs=[user_input, item_input], outputs=prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"functional_47\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "user (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "item (InputLayer)               [(None, 1)]          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "user_embed (Embedding)          (None, 1, 24)        144960      user[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "item_embed (Embedding)          (None, 1, 24)        88944       item[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "flatten_71 (Flatten)            (None, 24)           0           user_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "flatten_72 (Flatten)            (None, 24)           0           item_embed[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_72 (Tens [(None, 24, 1)]      0           flatten_71[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_73 (Tens [(None, 1, 24)]      0           flatten_72[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_BatchMatMulV2_24 (T [(None, 24, 24)]     0           tf_op_layer_ExpandDims_72[0][0]  \n",
      "                                                                 tf_op_layer_ExpandDims_73[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_ExpandDims_74 (Tens [(None, 24, 24, 1)]  0           tf_op_layer_BatchMatMulV2_24[0][0\n",
      "__________________________________________________________________________________________________\n",
      "conv2d_435 (Conv2D)             (None, 24, 24, 8)    80          tf_op_layer_ExpandDims_74[0][0]  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_367 (ReLU)                (None, 24, 24, 8)    0           conv2d_435[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_367 (BatchN (None, 24, 24, 8)    32          re_lu_367[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_436 (Conv2D)             (None, 24, 24, 8)    584         batch_normalization_367[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_184 (Add)                   (None, 24, 24, 8)    0           tf_op_layer_ExpandDims_74[0][0]  \n",
      "                                                                 conv2d_436[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_368 (ReLU)                (None, 24, 24, 8)    0           add_184[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_368 (BatchN (None, 24, 24, 8)    32          re_lu_368[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_437 (Conv2D)             (None, 24, 24, 8)    584         batch_normalization_368[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_369 (ReLU)                (None, 24, 24, 8)    0           conv2d_437[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_369 (BatchN (None, 24, 24, 8)    32          re_lu_369[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_438 (Conv2D)             (None, 24, 24, 8)    584         batch_normalization_369[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_185 (Add)                   (None, 24, 24, 8)    0           batch_normalization_368[0][0]    \n",
      "                                                                 conv2d_438[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_370 (ReLU)                (None, 24, 24, 8)    0           add_185[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_370 (BatchN (None, 24, 24, 8)    32          re_lu_370[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_439 (Conv2D)             (None, 12, 12, 8)    584         batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_371 (ReLU)                (None, 12, 12, 8)    0           conv2d_439[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_371 (BatchN (None, 12, 12, 8)    32          re_lu_371[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_441 (Conv2D)             (None, 12, 12, 8)    72          batch_normalization_370[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_440 (Conv2D)             (None, 12, 12, 8)    584         batch_normalization_371[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_186 (Add)                   (None, 12, 12, 8)    0           conv2d_441[0][0]                 \n",
      "                                                                 conv2d_440[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_372 (ReLU)                (None, 12, 12, 8)    0           add_186[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_372 (BatchN (None, 12, 12, 8)    32          re_lu_372[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_442 (Conv2D)             (None, 12, 12, 8)    584         batch_normalization_372[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_373 (ReLU)                (None, 12, 12, 8)    0           conv2d_442[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_373 (BatchN (None, 12, 12, 8)    32          re_lu_373[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_443 (Conv2D)             (None, 12, 12, 8)    584         batch_normalization_373[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_187 (Add)                   (None, 12, 12, 8)    0           batch_normalization_372[0][0]    \n",
      "                                                                 conv2d_443[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_374 (ReLU)                (None, 12, 12, 8)    0           add_187[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_374 (BatchN (None, 12, 12, 8)    32          re_lu_374[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_444 (Conv2D)             (None, 12, 12, 8)    584         batch_normalization_374[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_375 (ReLU)                (None, 12, 12, 8)    0           conv2d_444[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_375 (BatchN (None, 12, 12, 8)    32          re_lu_375[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_445 (Conv2D)             (None, 12, 12, 8)    584         batch_normalization_375[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_188 (Add)                   (None, 12, 12, 8)    0           batch_normalization_374[0][0]    \n",
      "                                                                 conv2d_445[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_376 (ReLU)                (None, 12, 12, 8)    0           add_188[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_376 (BatchN (None, 12, 12, 8)    32          re_lu_376[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_446 (Conv2D)             (None, 6, 6, 8)      584         batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_377 (ReLU)                (None, 6, 6, 8)      0           conv2d_446[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_377 (BatchN (None, 6, 6, 8)      32          re_lu_377[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_448 (Conv2D)             (None, 6, 6, 8)      72          batch_normalization_376[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_447 (Conv2D)             (None, 6, 6, 8)      584         batch_normalization_377[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_189 (Add)                   (None, 6, 6, 8)      0           conv2d_448[0][0]                 \n",
      "                                                                 conv2d_447[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_378 (ReLU)                (None, 6, 6, 8)      0           add_189[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_378 (BatchN (None, 6, 6, 8)      32          re_lu_378[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_449 (Conv2D)             (None, 6, 6, 8)      584         batch_normalization_378[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_379 (ReLU)                (None, 6, 6, 8)      0           conv2d_449[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_379 (BatchN (None, 6, 6, 8)      32          re_lu_379[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_450 (Conv2D)             (None, 6, 6, 8)      584         batch_normalization_379[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_190 (Add)                   (None, 6, 6, 8)      0           batch_normalization_378[0][0]    \n",
      "                                                                 conv2d_450[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_380 (ReLU)                (None, 6, 6, 8)      0           add_190[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_380 (BatchN (None, 6, 6, 8)      32          re_lu_380[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_451 (Conv2D)             (None, 6, 6, 8)      584         batch_normalization_380[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_381 (ReLU)                (None, 6, 6, 8)      0           conv2d_451[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_381 (BatchN (None, 6, 6, 8)      32          re_lu_381[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_452 (Conv2D)             (None, 6, 6, 8)      584         batch_normalization_381[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_191 (Add)                   (None, 6, 6, 8)      0           batch_normalization_380[0][0]    \n",
      "                                                                 conv2d_452[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_382 (ReLU)                (None, 6, 6, 8)      0           add_191[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_382 (BatchN (None, 6, 6, 8)      32          re_lu_382[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_453 (Conv2D)             (None, 3, 3, 8)      584         batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_383 (ReLU)                (None, 3, 3, 8)      0           conv2d_453[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_383 (BatchN (None, 3, 3, 8)      32          re_lu_383[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_455 (Conv2D)             (None, 3, 3, 8)      72          batch_normalization_382[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_454 (Conv2D)             (None, 3, 3, 8)      584         batch_normalization_383[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "add_192 (Add)                   (None, 3, 3, 8)      0           conv2d_455[0][0]                 \n",
      "                                                                 conv2d_454[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_384 (ReLU)                (None, 3, 3, 8)      0           add_192[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_384 (BatchN (None, 3, 3, 8)      32          re_lu_384[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten_73 (Flatten)            (None, 72)           0           batch_normalization_384[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "prediction (Dense)              (None, 1)            73          flatten_73[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 244,777\n",
      "Trainable params: 244,489\n",
      "Non-trainable params: 288\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "\n",
    "topK = 10\n",
    "\n",
    "#model = get_OuterProductmodel(num_users, num_items, 64)\n",
    "model = get_ResidualModel(num_users, num_items, 24)\n",
    "model.compile(optimizer=Adam(0.05), loss='binary_crossentropy')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluateNDCG(ranked_list, target_item):\n",
    "    for i in range(len(ranked_list)):\n",
    "        if ranked_list[i] == target_item:\n",
    "            return math.log(2) / math.log(i + 2)\n",
    "  \n",
    "    return 0\n",
    "\n",
    "def hitRate(ranked_list, target_item):\n",
    "    for rank in ranked_list:\n",
    "        if target_item == rank:\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "# This method calculates all the evaluation metrics. Individual methods are called from here.\n",
    "def evaluate(model, testPosRatings, testNegRatings, N):\n",
    "    hits = []\n",
    "    ndcgs = []\n",
    "    for i in range(len(testPosRatings)):\n",
    "        hit, ncdg = evaluate_one(model, testPosRatings[i], testNegRatings[i], N)\n",
    "        hits.append(hit)\n",
    "        ndcgs.append(ncdg)\n",
    "        \n",
    "    return np.array(hits).mean(), np.array(ndcgs).mean()\n",
    "\n",
    "def evaluate_one(model, posRating, negRatings, N):\n",
    "    user = posRating[0]\n",
    "    movie = posRating[1]\n",
    "    negRatings.append(movie)\n",
    "\n",
    "    user_input = np.full(len(negRatings), user)\n",
    "\n",
    "    predictions = model.predict([user_input, np.array(negRatings)], batch_size = 100)\n",
    "\n",
    "  # associate item with predictions\n",
    "    items = {}\n",
    "    for i in range(len(predictions)):\n",
    "        items[negRatings[i]] = predictions[i]\n",
    "    negRatings.pop()\n",
    "\n",
    "    rankedList = heapq.nlargest(N, items, items.get)\n",
    "    ndcg = evaluateNDCG(rankedList, movie)\n",
    "    hit = hitRate(rankedList, movie)\n",
    "\n",
    "    return hit, ndcg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and Save Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.3650\n",
      "Epoch 1 Hit Rate: 0.45480132450331123 NCDG: 0.24982109448047418\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.3361\n",
      "Epoch 2 Hit Rate: 0.5096026490066226 NCDG: 0.2814270169691672\n",
      "9709/9709 [==============================] - 158s 16ms/step - loss: 0.3187\n",
      "Epoch 3 Hit Rate: 0.5536423841059602 NCDG: 0.3071710961392581\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.3024\n",
      "Epoch 4 Hit Rate: 0.5799668874172186 NCDG: 0.32444587037399497\n",
      "9709/9709 [==============================] - 158s 16ms/step - loss: 0.2920\n",
      "Epoch 5 Hit Rate: 0.5890728476821192 NCDG: 0.33553364705193034\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2859\n",
      "Epoch 6 Hit Rate: 0.6009933774834437 NCDG: 0.33621539265558265\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2805\n",
      "Epoch 7 Hit Rate: 0.6096026490066225 NCDG: 0.3506172474391914\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2767\n",
      "Epoch 8 Hit Rate: 0.6170529801324504 NCDG: 0.35425243717650373\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2739\n",
      "Epoch 9 Hit Rate: 0.6233443708609272 NCDG: 0.3573203386103574\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2710\n",
      "Epoch 10 Hit Rate: 0.6279801324503311 NCDG: 0.3619850322706931\n",
      "9709/9709 [==============================] - 160s 16ms/step - loss: 0.2686\n",
      "Epoch 11 Hit Rate: 0.6339403973509934 NCDG: 0.3648148696684129\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2671\n",
      "Epoch 12 Hit Rate: 0.6233443708609272 NCDG: 0.36180737476173885\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2651\n",
      "Epoch 13 Hit Rate: 0.6367549668874172 NCDG: 0.36675550071184754\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2641\n",
      "Epoch 14 Hit Rate: 0.6395695364238411 NCDG: 0.37323754806888665\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2627\n",
      "Epoch 15 Hit Rate: 0.6350993377483444 NCDG: 0.3673048562734364\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2611\n",
      "Epoch 16 Hit Rate: 0.6413907284768212 NCDG: 0.37373337597896683\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2601\n",
      "Epoch 17 Hit Rate: 0.6403973509933775 NCDG: 0.3714433463914931\n",
      "9709/9709 [==============================] - 163s 17ms/step - loss: 0.2587\n",
      "Epoch 18 Hit Rate: 0.6425496688741722 NCDG: 0.3727053336483214\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2577\n",
      "Epoch 19 Hit Rate: 0.6410596026490066 NCDG: 0.3705640676265315\n",
      "9709/9709 [==============================] - 159s 16ms/step - loss: 0.2568\n",
      "Epoch 20 Hit Rate: 0.6402317880794702 NCDG: 0.36836914108470054\n",
      "Best Iteration 18:  HR = 0.6425, NDCG = 0.3727. \n",
      "The best Outer Product model is saved to OP_model.h5\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "best_hr = 0\n",
    "best_ncdg = 0\n",
    "best_epoch = -1\n",
    "model_path = \"ResNet_Model.h5\"\n",
    "\n",
    "'''\n",
    "hit_rate, ncdg = evaluate(model, test_ratings, test_negatives, N = 10)\n",
    "print('Initial Model', 'Hit Rate:', hit_rate, 'NCDG:', ncdg)\n",
    "\n",
    "user_input, item_input, labels = get_train_instances(train, num_negatives = 4)\n",
    "model.evaluate([np.array(user_input), np.array(item_input)],\n",
    "                      np.array(labels),\n",
    "                      batch_size = 256)\n",
    "'''\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    user_input, item_input, labels = get_train_instances(train, num_negatives = 4)\n",
    "\n",
    "    hist = model.fit([np.array(user_input), np.array(item_input)],\n",
    "                      np.array(labels),\n",
    "                      batch_size = 512, epochs = 1)\n",
    "\n",
    "    hit_rate, ncdg = evaluate(model, test_ratings, test_negatives, N = 10)\n",
    "    print('Epoch', epoch, 'Hit Rate:', hit_rate, 'NCDG:', ncdg)\n",
    "    model.save(model_path)\n",
    "\n",
    "    if hit_rate > best_hr:\n",
    "        best_hr, best_ncdg, best_iter = hit_rate, ncdg, epoch\n",
    "        model.save(model_path, overwrite=True)\n",
    "\n",
    "print(\"Best Iteration %d:  HR = %.4f, NDCG = %.4f. \" %(best_iter, best_hr, best_ncdg))\n",
    "print(\"The best Outer Product model is saved to %s\" %(model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
